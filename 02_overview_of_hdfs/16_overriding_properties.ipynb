{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overriding Properties\n",
    "\n",
    "Let us understand how we can override the properties while running `hdfs dfs` or `hadoop fs` commands.\n",
    "* We can change any property which is not defined as final in **core-site.xml** or **hdfs-site.xml**.\n",
    "* We can change `blocksize` as well as `replication` while copying the files. We can also change the replication factor after copying the files as well.\n",
    "* We can either pass individual properties using `-D` or bunch of properties by passing xml similar to **core-site.xml** or **hdfs-site.xml** as part of `--conf`.\n",
    "* Let's copy a file **/data/crime/csv/rows.csv** with default values. The file is splitted into 12 blocks with 2 copies each (as our default blocksize is 128 MB and replication factor is 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -mkdir -p /user/${USER}/crime/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1470260\n",
      "-rw-r--r-- 1 root root 1505540526 Aug  8  2017 rows.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "ls -ltr /data/crime/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -put /data/crime/csv/rows.csv /user/${USER}/crime/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -stat %r /user/${USER}/crime/csv/rows.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134217728\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -stat %o /user/${USER}/crime/csv/rows.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1505540526\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -stat %b /user/${USER}/crime/csv/rows.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/01/07 06:27:55 INFO fs.TrashPolicyDefault: Moved: 'hdfs://nn01.itversity.com:8020/user/itversity/crime/csv/rows.csv' to trash at: hdfs://nn01.itversity.com:8020/user/itversity/.Trash/Current/user/itversity/crime/csv/rows.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -rm -R -skpTrash /user/${USER}/crime/csv/rows.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -Ddfs.blocksize=64M -Ddfs.replication=3 -put /data/crime/csv/rows.csv /user/${USER}/crime/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
