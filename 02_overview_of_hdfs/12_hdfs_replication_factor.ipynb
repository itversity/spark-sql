{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS Replication Factor\n",
    "\n",
    "Let us get an overview of replication factor - another important building block of HDFS.\n",
    "* While blocksize drives distribution of large files, replication factor drives reliability of the blocks.\n",
    "* If we only have one copy of each block for a given file and if the node goes down, then the data in the files is not readable.\n",
    "* HDFS replication mitigates this by maintaining multiple copies of each block.\n",
    "* Keep in mind that the default replication factor is **3** unless we override it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls -h /public/retail_db/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs fsck /public/retail_db/orders/part-00000 \\\n",
    "    -files \\\n",
    "    -blocks \\\n",
    "    -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As part of our lab cluster we maintain 2 copies of each block.* In production implementations, typically we have 3 copies with rack awareness enabled.\n",
    "* The default replication factor is 3 and it is set as part of hdfs-site.xml. In our case we have overridden to save the storage.\n",
    "* The property name is `dfs.replication`.\n",
    "* If the file size is smaller than default blocksize (128 MB), then there will be only one block as per the size of the file.\n",
    "* In a typical configuration with n replication factor, there will not any down time even if n - 1 nodes go down in the cluster.\n",
    "* If replication factor is 3, cluster will be stable even if 2 of the nodes goes down in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "grep -B 1 -A 3 replication /etc/hadoop/conf/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us determine overall size occupied by `/data/retail_db/orders/part-00000` when it is copied to HDFS.\n",
    "* It occupies 5.8 MB storage in HDFS (as our replication factor is 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "ls -lhtr /data/retail_db/orders/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -help stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -stat %r /user/${USER}/retail_db/orders/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -stat %o /user/${USER}/retail_db/orders/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -stat %b /user/${USER}/retail_db/orders/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls -h /user/${USER}/retail_db/orders/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It occupies 4.8 GB storage in HDFS as our replication factor is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "ls -lhtr /data/yelp-dataset-json/yelp_academic_dataset_user.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can validate properties of the file using `stat` command. The file is available in HDFS under `/public/yelp-dataset-json/yelp_academic_dataset_user.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs fsck /public/yelp-dataset-json/yelp_academic_dataset_user.json \\\n",
    "    -files \\\n",
    "    -blocks \\\n",
    "    -locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
