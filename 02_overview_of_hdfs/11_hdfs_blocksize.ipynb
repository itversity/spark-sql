{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS Blocksize\n",
    "\n",
    "Let us get into details related to blocksize in HDFS.\n",
    "* HDFS stands for Hadoop Distributed File System.\n",
    "* It means the large files will be physically stored on multiple nodes in distributed fashion.\n",
    "* Let us review the `hdfs fsck` output of `/public/randomtextwriter/part-m-00000`. The file is approximately 1 GB in size and you will see 9 files.\n",
    "  * 8 files of size 128 MB\n",
    "  * 1 file of size 28 MB approximately\n",
    "* It means a file of size 1 GB 28 MB is stored in 9 blocks. It is due to the default block size which is 128 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls -h /public/randomtextwriter/part-m-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs fsck /public/randomtextwriter/part-m-00000 \\\n",
    "    -files \\\n",
    "    -blocks \\\n",
    "    -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The default block size is 128 MB and it is set as part of hdfs-site.xml.\n",
    "* The property name is `dfs.blocksize`.\n",
    "* If the file size is smaller than default blocksize (128 MB), then there will be only one block as per the size of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "cat /etc/hadoop/conf/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us determine the number of blocks for `/data/retail_db/orders/part-00000`. If we store this file of size 2.9 MB in HDFS, there will be one block associated with it as size of the file is less than the block size.\n",
    "* It occupies 2.9 MB storage in HDFS (assuming replication factor as 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "ls -lhtr /data/retail_db/orders/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us determine the number of blocks for `/data/yelp-dataset-json/yelp_academic_dataset_user.json`. If we store this file of size 2.4 GB in HDFS, there will be 19 blocks associated with it\n",
    "  * 18 128 MB Files\n",
    "  * 1 ~69 MB File\n",
    "* It occupies 2.4 GB storage in HDFS (assuming replication factor as 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "ls -lhtr /data/yelp-dataset-json/yelp_academic_dataset_user.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can validate by using `hdfs fsck` command against the same file in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs fsck /public/yelp-dataset-json/yelp_academic_dataset_user.json \\\n",
    "    -files \\\n",
    "    -blocks \\\n",
    "    -locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
