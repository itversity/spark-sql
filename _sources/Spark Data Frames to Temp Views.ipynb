{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    enableHiveSupport(). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/${username}/warehouse\"). \\\n",
    "    appName(f'{username} | Python - Basic Transformations'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT current_database()\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"USE {username}_retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+-----------+\n",
      "|        database|tableName|isTemporary|\n",
      "+----------------+---------+-----------+\n",
      "|itversity_retail|   orders|      false|\n",
      "+----------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gw01.itversity.com:38683\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.2.6.5.0-292</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>itversity | Python - Basic Transformations</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f93097be7b8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameReader at 0x7f93097b6f28>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 2999944 Feb 20  2017 /data/retail_db/orders/part-00000\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "ls -ltr /data/retail_db/orders/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68874,2014-07-03 00:00:00.0,1601,COMPLETE\n",
      "68875,2014-07-04 00:00:00.0,10637,ON_HOLD\n",
      "68876,2014-07-06 00:00:00.0,4124,COMPLETE\n",
      "68877,2014-07-07 00:00:00.0,9692,ON_HOLD\n",
      "68878,2014-07-08 00:00:00.0,6753,COMPLETE\n",
      "68879,2014-07-09 00:00:00.0,778,COMPLETE\n",
      "68880,2014-07-13 00:00:00.0,1117,COMPLETE\n",
      "68881,2014-07-19 00:00:00.0,2518,PENDING_PAYMENT\n",
      "68882,2014-07-22 00:00:00.0,10000,ON_HOLD\n",
      "68883,2014-07-23 00:00:00.0,5533,COMPLETE\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "tail /data/retail_db/orders/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "014-06-12 00:00:00.0,4229,PENDING\n",
      "68861,2014-06-13 00:00:00.0,3031,PENDING_PAYMENT\n",
      "68862,2014-06-15 00:00:00.0,7326,PROCESSING\n",
      "68863,2014-06-16 00:00:00.0,3361,CLOSED\n",
      "68864,2014-06-18 00:00:00.0,9634,ON_HOLD\n",
      "68865,2014-06-19 00:00:00.0,4567,SUSPECTED_FRAUD\n",
      "68866,2014-06-20 00:00:00.0,3890,PENDING_PAYMENT\n",
      "68867,2014-06-23 00:00:00.0,869,CANCELED\n",
      "68868,2014-06-24 00:00:00.0,10184,PENDING\n",
      "68869,2014-06-25 00:00:00.0,7456,PROCESSING\n",
      "68870,2014-06-26 00:00:00.0,3343,COMPLETE\n",
      "68871,2014-06-28 00:00:00.0,4960,PENDING\n",
      "68872,2014-06-29 00:00:00.0,3354,COMPLETE\n",
      "68873,2014-06-30 00:00:00.0,4545,PENDING\n",
      "68874,2014-07-03 00:00:00.0,1601,COMPLETE\n",
      "68875,2014-07-04 00:00:00.0,10637,ON_HOLD\n",
      "68876,2014-07-06 00:00:00.0,4124,COMPLETE\n",
      "68877,2014-07-07 00:00:00.0,9692,ON_HOLD\n",
      "68878,2014-07-08 00:00:00.0,6753,COMPLETE\n",
      "68879,2014-07-09 00:00:00.0,778,COMPLETE\n",
      "68880,2014-07-13 00:00:00.0,1117,COMPLETE\n",
      "68881,2014-07-19 00:00:00.0,2518,PENDING_PAYMENT\n",
      "68882,2014-07-22 00:00:00.0,10000,ON_HOLD\n",
      "68883,2014-07-23 00:00:00.0,5533,COMPLETE\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -tail /public/retail_db/orders/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mescape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignoreLeadingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnullValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnanValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpositiveInf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegativeInf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdateFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtimestampFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmaxColumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmaxCharsPerColumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmaxMalformedLogPerPartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolumnNameOfCorruptRecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmultiLine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
       "\n",
       "This function will go through the input once to determine the input schema if\n",
       "``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
       "``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
       "\n",
       ":param path: string, or list of strings, for input path(s),\n",
       "             or RDD of Strings storing CSV rows.\n",
       ":param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
       "               or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
       ":param sep: sets a single character as a separator for each field and value.\n",
       "            If None is set, it uses the default value, ``,``.\n",
       ":param encoding: decodes the CSV files by the given encoding type. If None is set,\n",
       "                 it uses the default value, ``UTF-8``.\n",
       ":param quote: sets a single character used for escaping quoted values where the\n",
       "              separator can be part of the value. If None is set, it uses the default\n",
       "              value, ``\"``. If you would like to turn off quotations, you need to set an\n",
       "              empty string.\n",
       ":param escape: sets a single character used for escaping quotes inside an already\n",
       "               quoted value. If None is set, it uses the default value, ``\\``.\n",
       ":param comment: sets a single character used for skipping lines beginning with this\n",
       "                character. By default (None), it is disabled.\n",
       ":param header: uses the first line as names of columns. If None is set, it uses the\n",
       "               default value, ``false``.\n",
       ":param inferSchema: infers the input schema automatically from data. It requires one extra\n",
       "               pass over the data. If None is set, it uses the default value, ``false``.\n",
       ":param ignoreLeadingWhiteSpace: A flag indicating whether or not leading whitespaces from\n",
       "                                values being read should be skipped. If None is set, it\n",
       "                                uses the default value, ``false``.\n",
       ":param ignoreTrailingWhiteSpace: A flag indicating whether or not trailing whitespaces from\n",
       "                                 values being read should be skipped. If None is set, it\n",
       "                                 uses the default value, ``false``.\n",
       ":param nullValue: sets the string representation of a null value. If None is set, it uses\n",
       "                  the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
       "                  applies to all supported types including the string type.\n",
       ":param nanValue: sets the string representation of a non-number value. If None is set, it\n",
       "                 uses the default value, ``NaN``.\n",
       ":param positiveInf: sets the string representation of a positive infinity value. If None\n",
       "                    is set, it uses the default value, ``Inf``.\n",
       ":param negativeInf: sets the string representation of a negative infinity value. If None\n",
       "                    is set, it uses the default value, ``Inf``.\n",
       ":param dateFormat: sets the string that indicates a date format. Custom date formats\n",
       "                   follow the formats at ``java.text.SimpleDateFormat``. This\n",
       "                   applies to date type. If None is set, it uses the\n",
       "                   default value, ``yyyy-MM-dd``.\n",
       ":param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
       "                        formats follow the formats at ``java.text.SimpleDateFormat``.\n",
       "                        This applies to timestamp type. If None is set, it uses the\n",
       "                        default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
       ":param maxColumns: defines a hard limit of how many columns a record can have. If None is\n",
       "                   set, it uses the default value, ``20480``.\n",
       ":param maxCharsPerColumn: defines the maximum number of characters allowed for any given\n",
       "                          value being read. If None is set, it uses the default value,\n",
       "                          ``-1`` meaning unlimited length.\n",
       ":param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\n",
       "                                    If specified, it is ignored.\n",
       ":param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
       "             set, it uses the default value, ``PERMISSIVE``.\n",
       "\n",
       "        * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string                   into a field configured by ``columnNameOfCorruptRecord``, and sets other                   fields to ``null``. To keep corrupt records, an user can set a string type                   field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a                   schema does not have the field, it drops corrupt records during parsing.                   A record with less/more tokens than schema is not a corrupted record to CSV.                   When it meets a record having fewer tokens than the length of the schema,                   sets ``null`` to extra fields. When the record has more tokens than the                   length of the schema, it drops extra tokens.\n",
       "        * ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
       "        * ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
       "\n",
       ":param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
       "                                  created by ``PERMISSIVE`` mode. This overrides\n",
       "                                  ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
       "                                  it uses the value specified in\n",
       "                                  ``spark.sql.columnNameOfCorruptRecord``.\n",
       ":param multiLine: parse records, which may span multiple lines. If None is\n",
       "                  set, it uses the default value, ``false``.\n",
       ":param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
       "                                  the quote character. If None is set, the default value is\n",
       "                                  escape character when escape and quote characters are\n",
       "                                  different, ``\u0000`` otherwise.\n",
       "\n",
       ">>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
       ">>> df.dtypes\n",
       "[('_c0', 'string'), ('_c1', 'string')]\n",
       ">>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
       ">>> df2 = spark.read.csv(rdd)\n",
       ">>> df2.dtypes\n",
       "[('_c0', 'string'), ('_c1', 'string')]\n",
       "\n",
       ".. versionadded:: 2.0\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/hdp/current/spark2-client/python/pyspark/sql/readwriter.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.csv(\n",
    "    '/public/retail_db/orders',\n",
    "    schema='order_id INT, order_date STRING, order_customer_id INT, order_status STRING'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[order_id: int, order_date: string, order_customer_id: int, order_status: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------------+---------------+\n",
      "|order_id|order_date           |order_customer_id|order_status   |\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "|1       |2013-07-25 00:00:00.0|11599            |CLOSED         |\n",
      "|2       |2013-07-25 00:00:00.0|256              |PENDING_PAYMENT|\n",
      "|3       |2013-07-25 00:00:00.0|12111            |COMPLETE       |\n",
      "|4       |2013-07-25 00:00:00.0|8827             |CLOSED         |\n",
      "|5       |2013-07-25 00:00:00.0|11318            |COMPLETE       |\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.createOrReplaceTempView('orders_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+-----------+\n",
      "|        database|  tableName|isTemporary|\n",
      "+----------------+-----------+-----------+\n",
      "|itversity_retail|     orders|      false|\n",
      "|                |orders_temp|       true|\n",
      "+----------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   68883|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT count(1) FROM orders_temp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   68883|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT count(1) FROM orders o JOIN orders_temp ot\n",
    "    ON o.order_id = ot.order_id\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "INSERT OVERWRITE TABLE orders_parquet\n",
    "SELECT * FROM orders_temp\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
