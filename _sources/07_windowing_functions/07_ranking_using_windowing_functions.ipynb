{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking using Windowing Functions\n",
    "\n",
    "Let us see how we can assign ranks using different **rank** functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/NsqHuZbUrAw?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/NsqHuZbUrAw?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username = itversity\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "itversity"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val username = System.getProperty(\"user.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username = itversity\n",
       "spark = org.apache.spark.sql.SparkSession@32571fa2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@32571fa2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    config(\"spark.sql.warehouse.dir\", s\"/user/${username}/warehouse\").\n",
    "    enableHiveSupport.\n",
    "    appName(s\"${username} | Spark SQL - Windowing Functions\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we have to assign ranks globally, we just need to specify **ORDER BY**\n",
    "* If we have to assign ranks with in a key then we need to specify **PARTITION BY** and then **ORDER BY**.\n",
    "* By default **ORDER BY** will sort the data in ascending order. We can change the order by passing **DESC** after order by.\n",
    "* We have 3 main functions to assign ranks - `rank`, `dense_rank` and `row_number`. We will see the difference between the 3 in a moment.\n",
    "\n",
    "Here is an example to assign sparse ranks using the table daily_product_revenue with in each day based on revenue. We can use `rank` function to assign sparse ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++\n",
       "\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Magic sql failed to execute with error: \n",
       "execute, tree:\n",
       "Exchange hashpartitioning(order_date#5, 200)\n",
       "+- HiveTableScan [order_date#5, order_item_product_id#6, revenue#7], HiveTableRelation `itversity_retail`.`daily_product_revenue`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [order_date#5, order_item_product_id#6, revenue#7]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "  rank() OVER (\n",
    "    PARTITION BY order_date\n",
    "    ORDER BY revenue DESC\n",
    "  ) AS rnk\n",
    "FROM daily_product_revenue t\n",
    "ORDER BY order_date, revenue DESC\n",
    "LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Here is another example to assign ranks using employees data set with in each department. We can also use other functions such as `dense_rank` and `row_number` to assign ranks.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++\n",
       "\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "  employee_id,\n",
    "  department_id,\n",
    "  salary,\n",
    "  rank() OVER (\n",
    "    PARTITION BY department_id\n",
    "    ORDER BY salary DESC\n",
    "  ) rnk,\n",
    "  dense_rank() OVER (\n",
    "    PARTITION BY department_id\n",
    "    ORDER BY salary DESC\n",
    "  ) drnk,\n",
    "  row_number() OVER (\n",
    "    PARTITION BY department_id\n",
    "    ORDER BY salary DESC\n",
    "  ) rn\n",
    "FROM employees\n",
    "ORDER BY department_id, salary DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  employee_id,\n",
    "  department_id,\n",
    "  salary,\n",
    "  rank() OVER (\n",
    "    PARTITION BY department_id\n",
    "    ORDER BY salary DESC\n",
    "  ) rnk,\n",
    "  dense_rank() OVER (\n",
    "    PARTITION BY department_id\n",
    "    ORDER BY salary DESC\n",
    "  ) drnk,\n",
    "  row_number() OVER (\n",
    "    PARTITION BY department_id\n",
    "    ORDER BY salary DESC, employee_id\n",
    "  ) rn\n",
    "FROM employees\n",
    "ORDER BY department_id, salary DESC\n",
    "\"\"\").\n",
    "    show(100, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM employees ORDER BY salary LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Here is the example for global rank with out `PARTITION BY` clause.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT employee_id, salary,\n",
    "    dense_rank() OVER (ORDER BY salary DESC) AS drnk\n",
    "FROM employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us understand the difference between **rank**, **dense_rank** and **row_number**.\n",
    "\n",
    "* We can use either of the functions to generate ranks when there are no duplicates in the column based on which ranks are assigned.\n",
    "* When the column based on which ranks are assigned have duplicates then row_number should not be used as it generate unique number for each record with in the partition. For those duplicate values, the row number need not be same across multiple runs.\n",
    "* **rank** will skip the ranks in between if multiple people get the same rank while **dense_rank** will not skip the ranks based up on the number of times the value is repeated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
