{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations using Windowing Functions\n",
    "\n",
    "Let us see how we can perform aggregations with in a partition or group using Windowing/Analytics Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/peDMzBredoU?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val username = System.getProperty(\"user.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    config(\"spark.sql.warehouse.dir\", s\"/user/${username}/warehouse\").\n",
    "    enableHiveSupport.\n",
    "    appName(s\"${username} | Spark SQL - Windowing Functions\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For simple aggregations where we have to get grouping key and aggregated results we can use **GROUP BY**.\n",
    "* If we want to get the raw data along with aggregated results, then using **GROUP BY** is not possible or overly complicated.\n",
    "* Using aggregate functions with **OVER** Clause not only simplifies the process of writing query, but also better with respect to performance.\n",
    "* Let us take an example of getting employee salary percentage when compared to department salary expense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT employee_id, department_id, salary \n",
    "FROM employees \n",
    "ORDER BY department_id, salary\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let us write the query using `GROUP BY` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT department_id,\n",
    "       sum(salary) AS department_salary_expense\n",
    "FROM employees\n",
    "GROUP BY department_id\n",
    "ORDER BY department_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT e.employee_id, e.department_id, e.salary,\n",
    "       ae.department_salary_expense,\n",
    "       ae.avg_salary_expense\n",
    "FROM employees e JOIN (\n",
    "     SELECT department_id, \n",
    "            sum(salary) AS department_salary_expense,\n",
    "            avg(salary) AS avg_salary_expense\n",
    "     FROM employees\n",
    "     GROUP BY department_id\n",
    ") ae\n",
    "ON e.department_id = ae.department_id\n",
    "ORDER BY department_id, salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let us see how we can get it using Analytics/Windowing Functions. \n",
    "\n",
    "* We can use all standard aggregate functions such as `count`, `sum`, `min`, `max`, `avg` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT e.employee_id, e.department_id, e.salary,\n",
    "       sum(e.salary) \n",
    "         OVER (PARTITION BY e.department_id)\n",
    "         AS department_salary_expense\n",
    "FROM employees e\n",
    "ORDER BY e.department_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT e.employee_id, e.department_id, e.salary,\n",
    "    sum(e.salary) OVER (PARTITION BY e.department_id) AS sum_sal_expense,\n",
    "    avg(e.salary) OVER (PARTITION BY e.department_id) AS avg_sal_expense,\n",
    "    min(e.salary) OVER (PARTITION BY e.department_id) AS min_sal_expense,\n",
    "    max(e.salary) OVER (PARTITION BY e.department_id) AS max_sal_expense,\n",
    "    count(e.salary) OVER (PARTITION BY e.department_id) AS cnt_sal_expense\n",
    "FROM employees e\n",
    "ORDER BY e.department_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tables to get daily revenue\n",
    "\n",
    "Let us create couple of tables which will be used for the demonstrations of Windowing and Ranking functions.\n",
    "\n",
    "* We have **ORDERS** and **ORDER_ITEMS** tables.\n",
    "* Let us take care of computing daily revenue as well as daily product revenue.\n",
    "* As we will be using same data set several times, let us create the tables to pre compute the data.\n",
    "* **daily_revenue** will have the **order_date** and **revenue**, where data is aggregated using **order_date** as partition key.\n",
    "* **daily_product_revenue** will have **order_date**, **order_item_product_id** and **revenue**. In this case data is aggregated using **order_date** and **order_item_product_id** as partition keys.\n",
    "\n",
    "Let us create table to compute daily revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS daily_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE daily_revenue\n",
    "AS\n",
    "SELECT o.order_date,\n",
    "       round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * \n",
    "FROM daily_revenue\n",
    "ORDER BY order_date\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create table to compute daily product revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "USE itversity_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS daily_product_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE daily_product_revenue\n",
    "AS\n",
    "SELECT o.order_date,\n",
    "       oi.order_item_product_id,\n",
    "       round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date, oi.order_item_product_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * \n",
    "FROM daily_product_revenue\n",
    "ORDER BY order_date, order_item_product_id\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
